<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "id": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "label": "3D Gaussian Splatting for Real...", "shape": "dot", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering\nAuthor: B. Kerbl\nYear: 2023\nAbstract: Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with mul\ntiple photos or videos. However, achieving high visual quality still requires neural networks that a\nre costly to train and render, while recent faster methods inevitably trade off speed for quality. F\nor unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no c\nurrent method can achieve real-time display rates. We introduce three key elements that allow us to \nachieve state-of-the-art visual quality while maintaining competitive training times and importantly\n allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting f\nrom sparse points produced during camera calibration, we represent the scene with 3D Gaussians that \npreserve desirable properties of continuous volumetric radiance fields for scene optimization while \navoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density\n control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate repre\nsentation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports \nanisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate st\nate-of-the-art visual quality and real-time rendering on several established datasets...."}, {"color": "#97c2fc", "id": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "label": "Neural Point Catacaustics for ...", "shape": "dot", "title": "Neural Point Catacaustics for Novel-View Synthesis of Reflections\nAuthor: Georgios Kopanas\nYear: 2022\nAbstract: View-dependent effects such as reflections pose a substantial challenge for image-based and neural r\nendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-\nlinear reflection flows as the camera moves. We introduce a new point-based representation to comput\ne Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a s\net of casually-captured input photos. At the core of our method is a neural warp field that models c\natacaustic trajectories of reflections, so complex specular effects can be rendered using efficient \npoint splatting in conjunction with a neural renderer. One of our key contributions is the explicit \nrepresentation of reflections with a reflection point cloud which is displaced by the neural warp fi\neld, and a primary point cloud which is optimized to represent the rest of the scene. After a short \nmanual annotation step, our approach allows interactive high-quality renderings of novel views with \naccurate reflection flow. Additionally, the explicit representation of reflection flow supports seve\nral forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular \nobjects, reflection tracking across views, and comfortable stereo viewing. We provide the source cod\ne and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/..."}, {"color": "#97c2fc", "id": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "label": "Scalable neural indoor scene r...", "shape": "dot", "title": "Scalable neural indoor scene rendering\nAuthor: Xiuchao Wu\nYear: 2022\nAbstract: We propose a scalable neural scene reconstruction and rendering method to support distributed traini\nng and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appe\narances are trained in parallel through a background sampling strategy that augments each tile with \ndistant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view\n-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular \nhighlights, reflections). We leverage the phenomena that complex view-dependent scene reflections ca\nn be attributed to virtual lights underneath surfaces at the total ray distance to the source. This \nlets us handle sparse samplings of the input scene where reflection highlights do not always appear \nconsistently in input images. We show interactive free-viewpoint rendering results from five scenes,\n one of which covers an area of more than 100 m2. Experimental results show that our method produces\n higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry an\nd voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.g\nithub.io/papers/scalable-nisr...."}, {"color": "#97c2fc", "id": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "label": "Point-NeRF: Point-based Neural...", "shape": "dot", "title": "Point-NeRF: Point-based Neural Radiance Fields\nAuthor: Qiangeng Xu\nYear: 2022\nAbstract: Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but \nare optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-vi\new stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF co\nmbines the advantages of these two approaches by using neural 3D point clouds, with associated neura\nl features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural \npoint features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF\n can be initialized via direct inference of a pre-trained deep network to produce a neural point clo\nud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training\n time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and o\nutliers in such methods via a novel pruning and growing mechanism...."}, {"color": "#97c2fc", "id": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "label": "Instant neural graphics primit...", "shape": "dot", "title": "Instant neural graphics primitives with a multiresolution hash encoding\nAuthor: T. M\u00fcller\nYear: 2022\nAbstract: Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train\n and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a sma\nller network without sacrificing quality, thus significantly reducing the number of floating point a\nnd memory access operations: a small neural network is augmented by a multiresolution hash table of \ntrainable feature vectors whose values are optimized through stochastic gradient descent. The multir\nesolution structure allows the network to disambiguate hash collisions, making for a simple architec\nture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the\n whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute\n operations. We achieve a combined speedup of several orders of magnitude, enabling training of high\n-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at\n a resolution of 1920\u00d71080...."}, {"color": "#97c2fc", "id": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "label": "Plenoxels: Radiance Fields wit...", "shape": "dot", "title": "Plenoxels: Radiance Fields without Neural Networks\nAuthor: Alex Yu\nYear: 2021\nAbstract: We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels repr\nesent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized fro\nm calibrated images via gradient methods and regularization without any neural components. On standa\nrd, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fie\nlds with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels...."}, {"color": "#97c2fc", "id": "self.name=\u0027Point\u2010Based Neural Rendering with Per\u2010View Optimization\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027, self.year=2021,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010based pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis.\u0027, self.paper_id=\u00272ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027", "label": "Point\u2010Based Neural Rendering w...", "shape": "dot", "title": "Point\u2010Based Neural Rendering with Per\u2010View Optimization\nAuthor: Georgios Kopanas\nYear: 2021\nAbstract: There has recently been great interest in neural rendering methods. Some approaches use 3D geometry \nreconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while\n others directly learn a volumetric neural representation, but suffer from expensive training and in\nference. We introduce a general approach that is initialized with MVS, but allows further optimizati\non of scene properties in the space of input views, including depth and reprojected features, result\ning in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010\nbased pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth\n test and effective camera selection. We use these elements together in our neural renderer, that ou\ntperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipelin\ne can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis...."}, {"color": "#97c2fc", "id": "self.name=\u0027Efficient Geometry-aware 3D Generative Adversarial Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/7c0a7419114db2209c2f386bc1537e90417cf9d4\u0027, self.year=2021,self.authors=\u0027Eric Chan\u0027, self.abstract=\u0027Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.\u0027, self.paper_id=\u00277c0a7419114db2209c2f386bc1537e90417cf9d4\u0027", "label": "Efficient Geometry-aware 3D Ge...", "shape": "dot", "title": "Efficient Geometry-aware 3D Generative Adversarial Networks\nAuthor: Eric Chan\nYear: 2021\nAbstract: Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collec\ntions of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either \ncompute intensive or make approximations that are not 3D-consistent; the former limits quality and r\nesolution of the generated images and the latter adversely affects multi-view consistency and shape \nquality. In this work, we improve the computational efficiency and image quality of 3D GANs without \noverly relying on these approximations. We introduce an expressive hybrid explicit implicit network \narchitecture that, together with other design choices, synthesizes not only high-resolution multi-vi\new-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature \ngeneration and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generator\ns, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-a\nrt 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments...."}, {"color": "#97c2fc", "id": "self.name=\u0027TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/4dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027, self.year=2021,self.authors=\u0027Martin Piala\u0027, self.abstract=\u0027Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.\u0027, self.paper_id=\u00274dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027", "label": "TermiNeRF: Ray Termination Pre...", "shape": "dot", "title": "TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\nAuthor: Martin Piala\nYear: 2021\nAbstract: Volume rendering using neural fields has shown great promise in capturing and synthesizing novel vie\nws of 3D scenes. However, this type of approach requires querying the volume network at multiple poi\nnts along each viewing ray in order to render an image, resulting in very slow rendering times. In t\nhis paper, we present a method that overcomes this limitation by learning a direct mapping from came\nra rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. U\nsing this approach we are able to render, train and fine-tune a volumetrically-rendered neural field\n model an order of magnitude faster than standard approaches. Unlike existing methods, our approach \nworks with general volumes and can be trained end-to-end...."}, {"color": "#97c2fc", "id": "self.name=\u0027ADOP\u0027, self.url=\u0027https://www.semanticscholar.org/paper/6c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027, self.year=2021,self.authors=\u0027Darius R\u00fcckert\u0027, self.abstract=\u0027In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP\u0027, self.paper_id=\u00276c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027", "label": "ADOP", "shape": "dot", "title": "ADOP\nAuthor: Darius R\u00fcckert\nYear: 2021\nAbstract: In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like o\nther neural renderers, our system takes as input calibrated camera images and a proxy geometry of th\ne scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with lea\nrned feature vectors as colors and a deep neural network fills the remaining holes and shades each o\nutput pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows \nus to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our \npipeline contains a fully differentiable physically-based photometric camera model, including exposu\nre, white balance, and a camera response function. Following the idea of inverse rendering, we use o\nur renderer to refine its input in order to reduce inconsistencies and optimize the quality of its o\nutput. In particular, we can optimize structural parameters like the camera pose, lens distortions, \npoint positions and features, and a neural environment map, but also photometric parameters like cam\nera response function, vignetting, and per-image exposure and white balance. Because our pipeline in\ncludes photometric parameters, e.g. exposure and camera response function, our system can smoothly h\nandle input images with varying exposure and white balance, and generates high-dynamic range output.\n We show that due to the improved input, we can achieve high render quality, also for difficult inpu\nt, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a res\nult, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination \nwith the fast point rasterization, ADOP achieves real-time rendering rates even for models with well\n over 100M points. https://github.com/darglein/ADOP..."}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027"}, {"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "to": "self.name=\u0027Point\u2010Based Neural Rendering with Per\u2010View Optimization\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027, self.year=2021,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010based pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis.\u0027, self.paper_id=\u00272ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "to": "self.name=\u0027Efficient Geometry-aware 3D Generative Adversarial Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/7c0a7419114db2209c2f386bc1537e90417cf9d4\u0027, self.year=2021,self.authors=\u0027Eric Chan\u0027, self.abstract=\u0027Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.\u0027, self.paper_id=\u00277c0a7419114db2209c2f386bc1537e90417cf9d4\u0027"}, {"arrows": "to", "from": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "to": "self.name=\u0027TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/4dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027, self.year=2021,self.authors=\u0027Martin Piala\u0027, self.abstract=\u0027Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.\u0027, self.paper_id=\u00274dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027"}, {"arrows": "to", "from": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "to": "self.name=\u0027ADOP\u0027, self.url=\u0027https://www.semanticscholar.org/paper/6c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027, self.year=2021,self.authors=\u0027Darius R\u00fcckert\u0027, self.abstract=\u0027In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP\u0027, self.paper_id=\u00276c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"shape": "dot", "size": 40, "font": {"size": 14, "face": "arial", "multi": true, "align": "center"}}, "edges": {"color": {"inherit": true}, "smooth": false}, "physics": {"forceAtlas2Based": {"gravitationalConstant": -150, "springLength": 200}, "minVelocity": 0.75, "solver": "forceAtlas2Based"}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>