<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "id": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "label": "3D Gaussian Splatting for Real...", "shape": "dot", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering\nAuthor: B. Kerbl\nYear: 2023\nAbstract:Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "label": "Neural Point Catacaustics for ...", "shape": "dot", "title": "Neural Point Catacaustics for Novel-View Synthesis of Reflections\nAuthor: Georgios Kopanas\nYear: 2022\nAbstract:View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "label": "Scalable neural indoor scene r...", "shape": "dot", "title": "Scalable neural indoor scene rendering\nAuthor: Xiuchao Wu\nYear: 2022\nAbstract:We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Variable Bitrate Neural Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292\u0027, self.year=2022,self.authors=\u0027Towaki Takikawa\u0027, self.abstract=\u0027Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\u0027, self.paper_id=\u0027d538645ba41ae2df38e63dfdd58958e0656a6292\u0027", "label": "Variable Bitrate Neural Fields", "shape": "dot", "title": "Variable Bitrate Neural Fields\nAuthor: Towaki Takikawa\nYear: 2022\nAbstract:Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "label": "Point-NeRF: Point-based Neural...", "shape": "dot", "title": "Point-NeRF: Point-based Neural Radiance Fields\nAuthor: Qiangeng Xu\nYear: 2022\nAbstract:Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "label": "Instant neural graphics primit...", "shape": "dot", "title": "Instant neural graphics primitives with a multiresolution hash encoding\nAuthor: T. M\u00fcller\nYear: 2022\nAbstract:Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/1cfbf598fee392d5d6bce37a2cf18971757151c7\u0027, self.year=2021,self.authors=\u0027Wanquan Feng\u0027, self.abstract=\u0027In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\u0027, self.paper_id=\u00271cfbf598fee392d5d6bce37a2cf18971757151c7\u0027", "label": "Neural Points: Point Cloud Rep...", "shape": "dot", "title": "Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\nAuthor: Wanquan Feng\nYear: 2021\nAbstract:In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "label": "Plenoxels: Radiance Fields wit...", "shape": "dot", "title": "Plenoxels: Radiance Fields without Neural Networks\nAuthor: Alex Yu\nYear: 2021\nAbstract:We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Point\u2010Based Neural Rendering with Per\u2010View Optimization\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027, self.year=2021,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010based pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis.\u0027, self.paper_id=\u00272ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027", "label": "Point\u2010Based Neural Rendering w...", "shape": "dot", "title": "Point\u2010Based Neural Rendering with Per\u2010View Optimization\nAuthor: Georgios Kopanas\nYear: 2021\nAbstract:There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010based pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027HyperNeRF\u0027, self.url=\u0027https://www.semanticscholar.org/paper/875129e6a54f18a537ba34845fda03701cacf388\u0027, self.year=2021,self.authors=\u0027Keunhong Park\u0027, self.abstract=\u0027Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.\u0027, self.paper_id=\u0027875129e6a54f18a537ba34845fda03701cacf388\u0027", "label": "HyperNeRF", "shape": "dot", "title": "HyperNeRF\nAuthor: Keunhong Park\nYear: 2021\nAbstract:Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Efficient Geometry-aware 3D Generative Adversarial Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/7c0a7419114db2209c2f386bc1537e90417cf9d4\u0027, self.year=2021,self.authors=\u0027Eric Chan\u0027, self.abstract=\u0027Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.\u0027, self.paper_id=\u00277c0a7419114db2209c2f386bc1537e90417cf9d4\u0027", "label": "Efficient Geometry-aware 3D Ge...", "shape": "dot", "title": "Efficient Geometry-aware 3D Generative Adversarial Networks\nAuthor: Eric Chan\nYear: 2021\nAbstract:Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027", "label": "Ref-NeRF: Structured View-Depe...", "shape": "dot", "title": "Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\nAuthor: Dor Verbin\nYear: 2021\nAbstract:Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027PU-EVA: An Edge-Vector based Approximation Solution for Flexible-scale Point Cloud Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/3e812667f61dd1bebaf9317d41d143c2142aeefa\u0027, self.year=2021,self.authors=\u0027Luqing Luo\u0027, self.abstract=\u0027High-quality point clouds have practical significance for point-based rendering, semantic understanding, and surface reconstruction. Upsampling sparse, noisy and non-uniform point clouds for a denser and more regular approximation of target objects is a desirable but challenging task. Most existing methods duplicate point features for upsampling, constraining the upsampling scales at a fixed rate. In this work, the arbitrary point clouds upsampling rates are achieved via edge-vector based affine combinations, and a novel design of Edge-Vector based Approximation for Flexible-scale Point clouds Upsampling (PU-EVA) is proposed. The edge-vector based approximation encodes neighboring connectivity via affine combinations based on edge vectors, and restricts the approximation error within a second-order term of Taylor\u2019s Expansion. Moreover, the EVA upsampling decouples the upsampling scales with network architecture, achieving the arbitrary upsampling rates in one-time training. Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA outperforms the state-of-the-arts in terms of proximity-to-surface, distribution uniformity, and geometric details preservation.\u0027, self.paper_id=\u00273e812667f61dd1bebaf9317d41d143c2142aeefa\u0027", "label": "PU-EVA: An Edge-Vector based A...", "shape": "dot", "title": "PU-EVA: An Edge-Vector based Approximation Solution for Flexible-scale Point Cloud Upsampling\nAuthor: Luqing Luo\nYear: 2021\nAbstract:High-quality point clouds have practical significance for point-based rendering, semantic understanding, and surface reconstruction. Upsampling sparse, noisy and non-uniform point clouds for a denser and more regular approximation of target objects is a desirable but challenging task. Most existing methods duplicate point features for upsampling, constraining the upsampling scales at a fixed rate. In this work, the arbitrary point clouds upsampling rates are achieved via edge-vector based affine combinations, and a novel design of Edge-Vector based Approximation for Flexible-scale Point clouds Upsampling (PU-EVA) is proposed. The edge-vector based approximation encodes neighboring connectivity via affine combinations based on edge vectors, and restricts the approximation error within a second-order term of Taylor\u2019s Expansion. Moreover, the EVA upsampling decouples the upsampling scales with network architecture, achieving the arbitrary upsampling rates in one-time training. Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA outperforms the state-of-the-arts in terms of proximity-to-surface, distribution uniformity, and geometric details preservation.\n"}, {"color": "#97c2fc", "id": "self.name=\u00273D Shapes Local Geometry Codes Learning with SDF\u0027, self.url=\u0027https://www.semanticscholar.org/paper/448d6d4beff2b1e15cbe2d18135f0ad5d67dc02e\u0027, self.year=2021,self.authors=\u0027Shun Yao\u0027, self.abstract=\u0027A signed distance function (SDF) as the 3D shape description is one of the most effective approaches to represent 3D geometry for rendering and reconstruction. Our work is inspired by the state-of-the-art method DeepSDF [17] that learns and analyzes the 3D shape as the iso-surface of its shell and this method has shown promising results especially in the 3D shape reconstruction and compression domain. In this paper, we consider the degeneration problem of reconstruction coming from the capacity decrease of the DeepSDF model, which approximates the SDF with a neural network and a single latent code. We propose Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF results by learning from a local shape geometry of the full 3D shape. We add an extra graph neural network to split the single transmittable latent code into a set of local latent codes distributed on the 3D shape. Mentioned latent codes are used to approximate the SDF in their local regions, which will alleviate the complexity of the approximation compared to the original DeepSDF. Furthermore, we introduce a new geometric loss function to facilitate the training of these local latent codes. Note that other local shape adjusting methods use the 3D voxel representation, which in turn is a problem highly difficult to solve or even is insolvable. In contrast, our architecture is based on graph processing implicitly and performs the learning regression process directly in the latent code space, thus make the proposed architecture more flexible and also simple for realization. Our experiments on 3D shape reconstruction demonstrate that our LGCL method can keep more details with a significantly smaller size of the SDF decoder and outperforms considerably the original DeepSDF method under the most important quantitative metrics.\u0027, self.paper_id=\u0027448d6d4beff2b1e15cbe2d18135f0ad5d67dc02e\u0027", "label": "3D Shapes Local Geometry Codes...", "shape": "dot", "title": "3D Shapes Local Geometry Codes Learning with SDF\nAuthor: Shun Yao\nYear: 2021\nAbstract:A signed distance function (SDF) as the 3D shape description is one of the most effective approaches to represent 3D geometry for rendering and reconstruction. Our work is inspired by the state-of-the-art method DeepSDF [17] that learns and analyzes the 3D shape as the iso-surface of its shell and this method has shown promising results especially in the 3D shape reconstruction and compression domain. In this paper, we consider the degeneration problem of reconstruction coming from the capacity decrease of the DeepSDF model, which approximates the SDF with a neural network and a single latent code. We propose Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF results by learning from a local shape geometry of the full 3D shape. We add an extra graph neural network to split the single transmittable latent code into a set of local latent codes distributed on the 3D shape. Mentioned latent codes are used to approximate the SDF in their local regions, which will alleviate the complexity of the approximation compared to the original DeepSDF. Furthermore, we introduce a new geometric loss function to facilitate the training of these local latent codes. Note that other local shape adjusting methods use the 3D voxel representation, which in turn is a problem highly difficult to solve or even is insolvable. In contrast, our architecture is based on graph processing implicitly and performs the learning regression process directly in the latent code space, thus make the proposed architecture more flexible and also simple for realization. Our experiments on 3D shape reconstruction demonstrate that our LGCL method can keep more details with a significantly smaller size of the SDF decoder and outperforms considerably the original DeepSDF method under the most important quantitative metrics.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/986efa2f1a8a1b9934eba23d10a4fbf20ada5c47\u0027, self.year=2021,self.authors=\u0027Yifan Zhao\u0027, self.abstract=\u0027Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner.\u0027, self.paper_id=\u0027986efa2f1a8a1b9934eba23d10a4fbf20ada5c47\u0027", "label": "SSPU-Net: Self-Supervised Poin...", "shape": "dot", "title": "SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering\nAuthor: Yifan Zhao\nYear: 2021\nAbstract:Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/4dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027, self.year=2021,self.authors=\u0027Martin Piala\u0027, self.abstract=\u0027Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.\u0027, self.paper_id=\u00274dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027", "label": "TermiNeRF: Ray Termination Pre...", "shape": "dot", "title": "TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\nAuthor: Martin Piala\nYear: 2021\nAbstract:Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027ADOP\u0027, self.url=\u0027https://www.semanticscholar.org/paper/6c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027, self.year=2021,self.authors=\u0027Darius R\u00fcckert\u0027, self.abstract=\u0027In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP\u0027, self.paper_id=\u00276c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027", "label": "ADOP", "shape": "dot", "title": "ADOP\nAuthor: Darius R\u00fcckert\nYear: 2021\nAbstract:In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Pulsar: Efficient Sphere-based Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/ffba7fc5a4b350d50c406b8e52a57cc31ac971dd\u0027, self.year=2021,self.authors=\u0027Christoph Lassner\u0027, self.abstract=\u0027We propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.\u0027, self.paper_id=\u0027ffba7fc5a4b350d50c406b8e52a57cc31ac971dd\u0027", "label": "Pulsar: Efficient Sphere-based...", "shape": "dot", "title": "Pulsar: Efficient Sphere-based Neural Rendering\nAuthor: Christoph Lassner\nYear: 2021\nAbstract:We propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition\u0027, self.url=\u0027https://www.semanticscholar.org/paper/15d599727cdc6acfddcf590d7c5ea242ffad40ff\u0027, self.year=2021,self.authors=\u0027Mark Boss\u0027, self.abstract=\u0027Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/\u0027, self.paper_id=\u002715d599727cdc6acfddcf590d7c5ea242ffad40ff\u0027", "label": "Neural-PIL: Neural Pre-Integra...", "shape": "dot", "title": "Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition\nAuthor: Mark Boss\nYear: 2021\nAbstract:Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Volume Rendering of Neural Implicit Surfaces\u0027, self.url=\u0027https://www.semanticscholar.org/paper/eded1f3acaba853499fd5a6b3de63fa9d5e0cef2\u0027, self.year=2021,self.authors=\u0027Lior Yariv\u0027, self.abstract=\"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace\u0027s cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.\", self.paper_id=\u0027eded1f3acaba853499fd5a6b3de63fa9d5e0cef2\u0027", "label": "Volume Rendering of Neural Imp...", "shape": "dot", "title": "Volume Rendering of Neural Implicit Surfaces\nAuthor: Lior Yariv\nYear: 2021\nAbstract:Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace\u0027s cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction\u0027, self.url=\u0027https://www.semanticscholar.org/paper/cf5647cb2613f5f697729eab567383006dcd4913\u0027, self.year=2021,self.authors=\u0027Peng Wang\u0027, self.abstract=\u0027We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.\u0027, self.paper_id=\u0027cf5647cb2613f5f697729eab567383006dcd4913\u0027", "label": "NeuS: Learning Neural Implicit...", "shape": "dot", "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction\nAuthor: Peng Wang\nYear: 2021\nAbstract:We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/03322c57507742b0127657893d815dce5f54f758\u0027, self.year=2022,self.authors=\u0027Jonathan Tremblay\u0027, self.abstract=\u0027We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\u0027, self.paper_id=\u002703322c57507742b0127657893d815dce5f54f758\u0027", "label": "RTMV: A Ray-Traced Multi-View ...", "shape": "dot", "title": "RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\nAuthor: Jonathan Tremblay\nYear: 2022\nAbstract:We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027NeRF\u2010Tex: Neural Reflectance Field Textures\u0027, self.url=\u0027https://www.semanticscholar.org/paper/a716f76329833b2d1835edda367656da1f591e76\u0027, self.year=2022,self.authors=\u0027Hendrik Baatz\u0027, self.abstract=\u0027We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\u0027, self.paper_id=\u0027a716f76329833b2d1835edda367656da1f591e76\u0027", "label": "NeRF\u2010Tex: Neural Reflectance F...", "shape": "dot", "title": "NeRF\u2010Tex: Neural Reflectance Field Textures\nAuthor: Hendrik Baatz\nYear: 2022\nAbstract:We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects\u0027, self.url=\u0027https://www.semanticscholar.org/paper/c3c087ba647465653a49a4be9213a39ac657af11\u0027, self.year=2021,self.authors=\u0027Jeffrey Ichnowski\u0027, self.abstract=\"The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF\u0027s view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.\", self.paper_id=\u0027c3c087ba647465653a49a4be9213a39ac657af11\u0027", "label": "Dex-NeRF: Using a Neural Radia...", "shape": "dot", "title": "Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects\nAuthor: Jeffrey Ichnowski\nYear: 2021\nAbstract:The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF\u0027s view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027ABO: Dataset and Benchmarks for Real-World 3D Object Understanding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/28fa2c85f891d4d22589d7a63f2c3a62bcb7b136\u0027, self.year=2021,self.authors=\u0027Jasmine Collins\u0027, self.abstract=\u0027We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with com-plex geometries and physically-based materials that cor-respond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.\u0027, self.paper_id=\u002728fa2c85f891d4d22589d7a63f2c3a62bcb7b136\u0027", "label": "ABO: Dataset and Benchmarks fo...", "shape": "dot", "title": "ABO: Dataset and Benchmarks for Real-World 3D Object Understanding\nAuthor: Jasmine Collins\nYear: 2021\nAbstract:We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with com-plex geometries and physically-based materials that cor-respond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Baking Neural Radiance Fields for Real-Time View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8bd70d3dcfa295d9710922c34c1a9eeb0be48b94\u0027, self.year=2021,self.authors=\u0027Peter Hedman\u0027, self.abstract=\u0027Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF\u2019s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF\u2019s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF\u2019s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.\u0027, self.paper_id=\u00278bd70d3dcfa295d9710922c34c1a9eeb0be48b94\u0027", "label": "Baking Neural Radiance Fields ...", "shape": "dot", "title": "Baking Neural Radiance Fields for Real-Time View Synthesis\nAuthor: Peter Hedman\nYear: 2021\nAbstract:Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF\u2019s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF\u2019s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF\u2019s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/21336e57dc2ab9ae2171a0f6c35f7d1aba584796\u0027, self.year=2021,self.authors=\u0027J. Barron\u0027, self.abstract=\u0027The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (\u00e0 la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF\u2019s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22\u00d7 faster.\u0027, self.paper_id=\u002721336e57dc2ab9ae2171a0f6c35f7d1aba584796\u0027", "label": "Mip-NeRF: A Multiscale Represe...", "shape": "dot", "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\nAuthor: J. Barron\nYear: 2021\nAbstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (\u00e0 la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF\u2019s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22\u00d7 faster.\n"}, {"color": "#97c2fc", "id": "self.name=\u0027DONeRF: Towards Real\u2010Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/792f25e5a2f167df962e0d25b6bee5474a86605f\u0027, self.year=2021,self.authors=\u0027Thomas Neff\u0027, self.abstract=\u0027The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high\u2010quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real\u2010time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real\u2010time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48\u00d7 compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching\u2010based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.\u0027, self.paper_id=\u0027792f25e5a2f167df962e0d25b6bee5474a86605f\u0027", "label": "DONeRF: Towards Real\u2010Time Rend...", "shape": "dot", "title": "DONeRF: Towards Real\u2010Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\nAuthor: Thomas Neff\nYear: 2021\nAbstract:The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high\u2010quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real\u2010time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real\u2010time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48\u00d7 compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching\u2010based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.\n"}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027"}, {"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027"}, {"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Variable Bitrate Neural Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292\u0027, self.year=2022,self.authors=\u0027Towaki Takikawa\u0027, self.abstract=\u0027Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\u0027, self.paper_id=\u0027d538645ba41ae2df38e63dfdd58958e0656a6292\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/1cfbf598fee392d5d6bce37a2cf18971757151c7\u0027, self.year=2021,self.authors=\u0027Wanquan Feng\u0027, self.abstract=\u0027In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\u0027, self.paper_id=\u00271cfbf598fee392d5d6bce37a2cf18971757151c7\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027"}, {"arrows": "to", "from": "self.name=\u0027Variable Bitrate Neural Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292\u0027, self.year=2022,self.authors=\u0027Towaki Takikawa\u0027, self.abstract=\u0027Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\u0027, self.paper_id=\u0027d538645ba41ae2df38e63dfdd58958e0656a6292\u0027", "to": "self.name=\u0027RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/03322c57507742b0127657893d815dce5f54f758\u0027, self.year=2022,self.authors=\u0027Jonathan Tremblay\u0027, self.abstract=\u0027We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\u0027, self.paper_id=\u002703322c57507742b0127657893d815dce5f54f758\u0027"}, {"arrows": "to", "from": "self.name=\u0027Variable Bitrate Neural Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292\u0027, self.year=2022,self.authors=\u0027Towaki Takikawa\u0027, self.abstract=\u0027Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\u0027, self.paper_id=\u0027d538645ba41ae2df38e63dfdd58958e0656a6292\u0027", "to": "self.name=\u0027NeRF\u2010Tex: Neural Reflectance Field Textures\u0027, self.url=\u0027https://www.semanticscholar.org/paper/a716f76329833b2d1835edda367656da1f591e76\u0027, self.year=2022,self.authors=\u0027Hendrik Baatz\u0027, self.abstract=\u0027We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\u0027, self.paper_id=\u0027a716f76329833b2d1835edda367656da1f591e76\u0027"}, {"arrows": "to", "from": "self.name=\u0027Variable Bitrate Neural Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292\u0027, self.year=2022,self.authors=\u0027Towaki Takikawa\u0027, self.abstract=\u0027Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; M\u00fcller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 \u00d7 and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\u0027, self.paper_id=\u0027d538645ba41ae2df38e63dfdd58958e0656a6292\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "to": "self.name=\u0027Point\u2010Based Neural Rendering with Per\u2010View Optimization\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027, self.year=2021,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi\u2010View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel\u2010view synthesis. A key element of our approach is our new differentiable point\u2010based pipeline, based on bi\u2010directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi\u2010view harmonization and stylization in addition to novel\u2010view synthesis.\u0027, self.paper_id=\u00272ac30b5ba83c1fabb0e864d4d3c93a70367accc6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "to": "self.name=\u0027HyperNeRF\u0027, self.url=\u0027https://www.semanticscholar.org/paper/875129e6a54f18a537ba34845fda03701cacf388\u0027, self.year=2021,self.authors=\u0027Keunhong Park\u0027, self.abstract=\u0027Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.\u0027, self.paper_id=\u0027875129e6a54f18a537ba34845fda03701cacf388\u0027"}, {"arrows": "to", "from": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "to": "self.name=\u0027Efficient Geometry-aware 3D Generative Adversarial Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/7c0a7419114db2209c2f386bc1537e90417cf9d4\u0027, self.year=2021,self.authors=\u0027Eric Chan\u0027, self.abstract=\u0027Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.\u0027, self.paper_id=\u00277c0a7419114db2209c2f386bc1537e90417cf9d4\u0027"}, {"arrows": "to", "from": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}, {"arrows": "to", "from": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "to": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/1cfbf598fee392d5d6bce37a2cf18971757151c7\u0027, self.year=2021,self.authors=\u0027Wanquan Feng\u0027, self.abstract=\u0027In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\u0027, self.paper_id=\u00271cfbf598fee392d5d6bce37a2cf18971757151c7\u0027", "to": "self.name=\u0027PU-EVA: An Edge-Vector based Approximation Solution for Flexible-scale Point Cloud Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/3e812667f61dd1bebaf9317d41d143c2142aeefa\u0027, self.year=2021,self.authors=\u0027Luqing Luo\u0027, self.abstract=\u0027High-quality point clouds have practical significance for point-based rendering, semantic understanding, and surface reconstruction. Upsampling sparse, noisy and non-uniform point clouds for a denser and more regular approximation of target objects is a desirable but challenging task. Most existing methods duplicate point features for upsampling, constraining the upsampling scales at a fixed rate. In this work, the arbitrary point clouds upsampling rates are achieved via edge-vector based affine combinations, and a novel design of Edge-Vector based Approximation for Flexible-scale Point clouds Upsampling (PU-EVA) is proposed. The edge-vector based approximation encodes neighboring connectivity via affine combinations based on edge vectors, and restricts the approximation error within a second-order term of Taylor\u2019s Expansion. Moreover, the EVA upsampling decouples the upsampling scales with network architecture, achieving the arbitrary upsampling rates in one-time training. Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA outperforms the state-of-the-arts in terms of proximity-to-surface, distribution uniformity, and geometric details preservation.\u0027, self.paper_id=\u00273e812667f61dd1bebaf9317d41d143c2142aeefa\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/1cfbf598fee392d5d6bce37a2cf18971757151c7\u0027, self.year=2021,self.authors=\u0027Wanquan Feng\u0027, self.abstract=\u0027In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\u0027, self.paper_id=\u00271cfbf598fee392d5d6bce37a2cf18971757151c7\u0027", "to": "self.name=\u00273D Shapes Local Geometry Codes Learning with SDF\u0027, self.url=\u0027https://www.semanticscholar.org/paper/448d6d4beff2b1e15cbe2d18135f0ad5d67dc02e\u0027, self.year=2021,self.authors=\u0027Shun Yao\u0027, self.abstract=\u0027A signed distance function (SDF) as the 3D shape description is one of the most effective approaches to represent 3D geometry for rendering and reconstruction. Our work is inspired by the state-of-the-art method DeepSDF [17] that learns and analyzes the 3D shape as the iso-surface of its shell and this method has shown promising results especially in the 3D shape reconstruction and compression domain. In this paper, we consider the degeneration problem of reconstruction coming from the capacity decrease of the DeepSDF model, which approximates the SDF with a neural network and a single latent code. We propose Local Geometry Code Learning (LGCL), a model that improves the original DeepSDF results by learning from a local shape geometry of the full 3D shape. We add an extra graph neural network to split the single transmittable latent code into a set of local latent codes distributed on the 3D shape. Mentioned latent codes are used to approximate the SDF in their local regions, which will alleviate the complexity of the approximation compared to the original DeepSDF. Furthermore, we introduce a new geometric loss function to facilitate the training of these local latent codes. Note that other local shape adjusting methods use the 3D voxel representation, which in turn is a problem highly difficult to solve or even is insolvable. In contrast, our architecture is based on graph processing implicitly and performs the learning regression process directly in the latent code space, thus make the proposed architecture more flexible and also simple for realization. Our experiments on 3D shape reconstruction demonstrate that our LGCL method can keep more details with a significantly smaller size of the SDF decoder and outperforms considerably the original DeepSDF method under the most important quantitative metrics.\u0027, self.paper_id=\u0027448d6d4beff2b1e15cbe2d18135f0ad5d67dc02e\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling\u0027, self.url=\u0027https://www.semanticscholar.org/paper/1cfbf598fee392d5d6bce37a2cf18971757151c7\u0027, self.year=2021,self.authors=\u0027Wanquan Feng\u0027, self.abstract=\u0027In this paper, we propose Neural Points, a novel point cloud representation and apply it to the arbitrary-factored upsampling task. Different from traditional point cloud representation where each point only represents a position or a local plane in the 3D space, each point in Neural Points represents a local continuous geometric shape via neural fields. Therefore, Neural Points contain more shape information and thus have a stronger representation ability. Neural Points is trained with surface containing rich geometric details, such that the trained model has enough expression ability for various shapes. Specifically, we extract deep local features on the points and construct neural fields through the local isomorphism between the 2D parametric domain and the 3D local patch. In the final, local neural fields are integrated together to form the global surface. Experimental results show that Neural Points has powerful representation ability and demonstrate excellent robustness and generalization ability. With Neural Points, we can resample point cloud with arbitrary resolutions, and it outperforms the state-of-the-art point cloud upsampling methods. Code is available at https://github.com/WanquanF/NeuralPoints.\u0027, self.paper_id=\u00271cfbf598fee392d5d6bce37a2cf18971757151c7\u0027", "to": "self.name=\u0027SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/986efa2f1a8a1b9934eba23d10a4fbf20ada5c47\u0027, self.year=2021,self.authors=\u0027Yifan Zhao\u0027, self.abstract=\u0027Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner.\u0027, self.paper_id=\u0027986efa2f1a8a1b9934eba23d10a4fbf20ada5c47\u0027"}, {"arrows": "to", "from": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "to": "self.name=\u0027TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/4dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027, self.year=2021,self.authors=\u0027Martin Piala\u0027, self.abstract=\u0027Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel\u2019s final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.\u0027, self.paper_id=\u00274dc162b18b911f28f2024035c8e8aa5d7fe35f76\u0027"}, {"arrows": "to", "from": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "to": "self.name=\u0027ADOP\u0027, self.url=\u0027https://www.semanticscholar.org/paper/6c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027, self.year=2021,self.authors=\u0027Darius R\u00fcckert\u0027, self.abstract=\u0027In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP\u0027, self.paper_id=\u00276c9eb27a3d9bd9ec3e6646d35702aa189a7ede80\u0027"}, {"arrows": "to", "from": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "to": "self.name=\u0027Pulsar: Efficient Sphere-based Neural Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/ffba7fc5a4b350d50c406b8e52a57cc31ac971dd\u0027, self.year=2021,self.authors=\u0027Christoph Lassner\u0027, self.abstract=\u0027We propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.\u0027, self.paper_id=\u0027ffba7fc5a4b350d50c406b8e52a57cc31ac971dd\u0027"}, {"arrows": "to", "from": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027", "to": "self.name=\u0027Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition\u0027, self.url=\u0027https://www.semanticscholar.org/paper/15d599727cdc6acfddcf590d7c5ea242ffad40ff\u0027, self.year=2021,self.authors=\u0027Mark Boss\u0027, self.abstract=\u0027Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/\u0027, self.paper_id=\u002715d599727cdc6acfddcf590d7c5ea242ffad40ff\u0027"}, {"arrows": "to", "from": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027", "to": "self.name=\u0027Volume Rendering of Neural Implicit Surfaces\u0027, self.url=\u0027https://www.semanticscholar.org/paper/eded1f3acaba853499fd5a6b3de63fa9d5e0cef2\u0027, self.year=2021,self.authors=\u0027Lior Yariv\u0027, self.abstract=\"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace\u0027s cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.\", self.paper_id=\u0027eded1f3acaba853499fd5a6b3de63fa9d5e0cef2\u0027"}, {"arrows": "to", "from": "self.name=\u0027Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/40c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027, self.year=2021,self.authors=\u0027Dor Verbin\u0027, self.abstract=\"Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF\u0027s parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model\u0027s internal representation of outgoing radiance is interpretable and useful for scene editing.\", self.paper_id=\u002740c8c8d8a41c16a0e017cc0d059fae9d346795f0\u0027", "to": "self.name=\u0027NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction\u0027, self.url=\u0027https://www.semanticscholar.org/paper/cf5647cb2613f5f697729eab567383006dcd4913\u0027, self.year=2021,self.authors=\u0027Peng Wang\u0027, self.abstract=\u0027We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.\u0027, self.paper_id=\u0027cf5647cb2613f5f697729eab567383006dcd4913\u0027"}, {"arrows": "to", "from": "self.name=\u0027RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/03322c57507742b0127657893d815dce5f54f758\u0027, self.year=2022,self.authors=\u0027Jonathan Tremblay\u0027, self.abstract=\u0027We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\u0027, self.paper_id=\u002703322c57507742b0127657893d815dce5f54f758\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/03322c57507742b0127657893d815dce5f54f758\u0027, self.year=2022,self.authors=\u0027Jonathan Tremblay\u0027, self.abstract=\u0027We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\u0027, self.paper_id=\u002703322c57507742b0127657893d815dce5f54f758\u0027", "to": "self.name=\u0027Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects\u0027, self.url=\u0027https://www.semanticscholar.org/paper/c3c087ba647465653a49a4be9213a39ac657af11\u0027, self.year=2021,self.authors=\u0027Jeffrey Ichnowski\u0027, self.abstract=\"The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF\u0027s view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.\", self.paper_id=\u0027c3c087ba647465653a49a4be9213a39ac657af11\u0027"}, {"arrows": "to", "from": "self.name=\u0027RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/03322c57507742b0127657893d815dce5f54f758\u0027, self.year=2022,self.authors=\u0027Jonathan Tremblay\u0027, self.abstract=\u0027We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.\u0027, self.paper_id=\u002703322c57507742b0127657893d815dce5f54f758\u0027", "to": "self.name=\u0027ABO: Dataset and Benchmarks for Real-World 3D Object Understanding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/28fa2c85f891d4d22589d7a63f2c3a62bcb7b136\u0027, self.year=2021,self.authors=\u0027Jasmine Collins\u0027, self.abstract=\u0027We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with com-plex geometries and physically-based materials that cor-respond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.\u0027, self.paper_id=\u002728fa2c85f891d4d22589d7a63f2c3a62bcb7b136\u0027"}, {"arrows": "to", "from": "self.name=\u0027NeRF\u2010Tex: Neural Reflectance Field Textures\u0027, self.url=\u0027https://www.semanticscholar.org/paper/a716f76329833b2d1835edda367656da1f591e76\u0027, self.year=2022,self.authors=\u0027Hendrik Baatz\u0027, self.abstract=\u0027We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\u0027, self.paper_id=\u0027a716f76329833b2d1835edda367656da1f591e76\u0027", "to": "self.name=\u0027Baking Neural Radiance Fields for Real-Time View Synthesis\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8bd70d3dcfa295d9710922c34c1a9eeb0be48b94\u0027, self.year=2021,self.authors=\u0027Peter Hedman\u0027, self.abstract=\u0027Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF\u2019s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF\u2019s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF\u2019s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.\u0027, self.paper_id=\u00278bd70d3dcfa295d9710922c34c1a9eeb0be48b94\u0027"}, {"arrows": "to", "from": "self.name=\u0027NeRF\u2010Tex: Neural Reflectance Field Textures\u0027, self.url=\u0027https://www.semanticscholar.org/paper/a716f76329833b2d1835edda367656da1f591e76\u0027, self.year=2022,self.authors=\u0027Hendrik Baatz\u0027, self.abstract=\u0027We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\u0027, self.paper_id=\u0027a716f76329833b2d1835edda367656da1f591e76\u0027", "to": "self.name=\u0027Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/21336e57dc2ab9ae2171a0f6c35f7d1aba584796\u0027, self.year=2021,self.authors=\u0027J. Barron\u0027, self.abstract=\u0027The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (\u00e0 la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF\u2019s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22\u00d7 faster.\u0027, self.paper_id=\u002721336e57dc2ab9ae2171a0f6c35f7d1aba584796\u0027"}, {"arrows": "to", "from": "self.name=\u0027NeRF\u2010Tex: Neural Reflectance Field Textures\u0027, self.url=\u0027https://www.semanticscholar.org/paper/a716f76329833b2d1835edda367656da1f591e76\u0027, self.year=2022,self.authors=\u0027Hendrik Baatz\u0027, self.abstract=\u0027We investigate the use of neural fields for modelling diverse mesoscale structures, such as fur, fabric and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF\u2010Tex), which jointly models the geometry of the material and its response to lighting. The NeRF\u2010Tex primitive can be instantiated over a base mesh to \u2018texture\u2019 it with the desired meso and microscale appearance. We condition the reflectance field on user\u2010defined parameters that control the appearance. A single NeRF texture thus captures an entire space of reflectance fields rather than one specific structure. This increases the gamut of appearances that can be modelled and provides a solution for combating repetitive texturing artifacts. We also demonstrate that NeRF textures naturally facilitate continuous level\u2010of\u2010detail rendering. Our approach unites the versatility and modelling power of neural networks with the artistic control needed for precise modelling of virtual scenes. While all our training data are currently synthetic, our work provides a recipe that can be further extended to extract complex, hard\u2010to\u2010model appearances from real images.\u0027, self.paper_id=\u0027a716f76329833b2d1835edda367656da1f591e76\u0027", "to": "self.name=\u0027DONeRF: Towards Real\u2010Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/792f25e5a2f167df962e0d25b6bee5474a86605f\u0027, self.year=2021,self.authors=\u0027Thomas Neff\u0027, self.abstract=\u0027The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high\u2010quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real\u2010time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real\u2010time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48\u00d7 compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching\u2010based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.\u0027, self.paper_id=\u0027792f25e5a2f167df962e0d25b6bee5474a86605f\u0027"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"shape": "dot", "size": 40, "font": {"size": 14, "face": "arial", "multi": true, "align": "center"}}, "edges": {"color": {"inherit": true}, "smooth": false}, "physics": {"forceAtlas2Based": {"gravitationalConstant": -50, "springLength": 100}, "minVelocity": 0.75, "solver": "forceAtlas2Based"}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>