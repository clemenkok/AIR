<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#97c2fc", "id": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "label": "3D Gaussian Splatting for Real...", "shape": "dot", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering\nAuthor: B. Kerbl\nYear: 2023\nAbstract: Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with mul\ntiple photos or videos. However, achieving high visual quality still requires neural networks that a\nre costly to train and render, while recent faster methods inevitably trade off speed for quality. F\nor unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no c\nurrent method can achieve real-time display rates. We introduce three key elements that allow us to \nachieve state-of-the-art visual quality while maintaining competitive training times and importantly\n allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting f\nrom sparse points produced during camera calibration, we represent the scene with 3D Gaussians that \npreserve desirable properties of continuous volumetric radiance fields for scene optimization while \navoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density\n control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate repre\nsentation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports \nanisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate st\nate-of-the-art visual quality and real-time rendering on several established datasets...."}, {"color": "#97c2fc", "id": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "label": "Neural Point Catacaustics for ...", "shape": "dot", "title": "Neural Point Catacaustics for Novel-View Synthesis of Reflections\nAuthor: Georgios Kopanas\nYear: 2022\nAbstract: View-dependent effects such as reflections pose a substantial challenge for image-based and neural r\nendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-\nlinear reflection flows as the camera moves. We introduce a new point-based representation to comput\ne Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a s\net of casually-captured input photos. At the core of our method is a neural warp field that models c\natacaustic trajectories of reflections, so complex specular effects can be rendered using efficient \npoint splatting in conjunction with a neural renderer. One of our key contributions is the explicit \nrepresentation of reflections with a reflection point cloud which is displaced by the neural warp fi\neld, and a primary point cloud which is optimized to represent the rest of the scene. After a short \nmanual annotation step, our approach allows interactive high-quality renderings of novel views with \naccurate reflection flow. Additionally, the explicit representation of reflection flow supports seve\nral forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular \nobjects, reflection tracking across views, and comfortable stereo viewing. We provide the source cod\ne and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/..."}, {"color": "#97c2fc", "id": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "label": "Scalable neural indoor scene r...", "shape": "dot", "title": "Scalable neural indoor scene rendering\nAuthor: Xiuchao Wu\nYear: 2022\nAbstract: We propose a scalable neural scene reconstruction and rendering method to support distributed traini\nng and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appe\narances are trained in parallel through a background sampling strategy that augments each tile with \ndistant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view\n-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular \nhighlights, reflections). We leverage the phenomena that complex view-dependent scene reflections ca\nn be attributed to virtual lights underneath surfaces at the total ray distance to the source. This \nlets us handle sparse samplings of the input scene where reflection highlights do not always appear \nconsistently in input images. We show interactive free-viewpoint rendering results from five scenes,\n one of which covers an area of more than 100 m2. Experimental results show that our method produces\n higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry an\nd voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.g\nithub.io/papers/scalable-nisr...."}, {"color": "#97c2fc", "id": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027", "label": "Point-NeRF: Point-based Neural...", "shape": "dot", "title": "Point-NeRF: Point-based Neural Radiance Fields\nAuthor: Qiangeng Xu\nYear: 2022\nAbstract: Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but \nare optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-vi\new stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF co\nmbines the advantages of these two approaches by using neural 3D point clouds, with associated neura\nl features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural \npoint features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF\n can be initialized via direct inference of a pre-trained deep network to produce a neural point clo\nud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training\n time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and o\nutliers in such methods via a novel pruning and growing mechanism...."}, {"color": "#97c2fc", "id": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027", "label": "Instant neural graphics primit...", "shape": "dot", "title": "Instant neural graphics primitives with a multiresolution hash encoding\nAuthor: T. M\u00fcller\nYear: 2022\nAbstract: Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train\n and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a sma\nller network without sacrificing quality, thus significantly reducing the number of floating point a\nnd memory access operations: a small neural network is augmented by a multiresolution hash table of \ntrainable feature vectors whose values are optimized through stochastic gradient descent. The multir\nesolution structure allows the network to disambiguate hash collisions, making for a simple architec\nture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the\n whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute\n operations. We achieve a combined speedup of several orders of magnitude, enabling training of high\n-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at\n a resolution of 1920\u00d71080...."}, {"color": "#97c2fc", "id": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027", "label": "Plenoxels: Radiance Fields wit...", "shape": "dot", "title": "Plenoxels: Radiance Fields without Neural Networks\nAuthor: Alex Yu\nYear: 2021\nAbstract: We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels repr\nesent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized fro\nm calibrated images via gradient methods and regularization without any neural components. On standa\nrd, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fie\nlds with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels...."}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027"}, {"arrows": "to", "from": "self.name=\u00273D Gaussian Splatting for Real-Time Radiance Field Rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/2cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027, self.year=\u00272023\u0027,self.authors=\u0027B. Kerbl\u0027, self.abstract=\u0027Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (\u2265 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\u0027, self.paper_id=\u00272cc1d857e86d5152ba7fe6a8355c2a0150cc280a\u0027", "to": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Point-NeRF: Point-based Neural Radiance Fields\u0027, self.url=\u0027https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85\u0027, self.year=2022,self.authors=\u0027Qiangeng Xu\u0027, self.abstract=\u0027Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30\u00d7 faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\u0027, self.paper_id=\u0027055e87ce418a83d6fd555b73aea0d838385dfa85\u0027"}, {"arrows": "to", "from": "self.name=\u0027Neural Point Catacaustics for Novel-View Synthesis of Reflections\u0027, self.url=\u0027https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027, self.year=2022,self.authors=\u0027Georgios Kopanas\u0027, self.abstract=\u0027View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\u0027, self.paper_id=\u0027d9e90c68ba1769d237e01c98c5de6a94d9c9087e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Instant neural graphics primitives with a multiresolution hash encoding\u0027, self.url=\u0027https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027, self.year=2022,self.authors=\u0027T. M\u00fcller\u0027, self.abstract=\u0027Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920\u00d71080.\u0027, self.paper_id=\u002760e69982ef2920596c6f31d6fd3ca5e9591f3db6\u0027"}, {"arrows": "to", "from": "self.name=\u0027Scalable neural indoor scene rendering\u0027, self.url=\u0027https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e\u0027, self.year=2022,self.authors=\u0027Xiuchao Wu\u0027, self.abstract=\u0027We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\u0027, self.paper_id=\u00278f16b0fcce850ba78270c2088cba64425524db2e\u0027", "to": "self.name=\u0027Plenoxels: Radiance Fields without Neural Networks\u0027, self.url=\u0027https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64\u0027, self.year=2021,self.authors=\u0027Alex Yu\u0027, self.abstract=\u0027We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\u0027, self.paper_id=\u0027e91f73aaef155391b5b07e6612f5346dea888f64\u0027"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"shape": "dot", "size": 40, "font": {"size": 14, "face": "arial", "multi": true, "align": "center"}}, "edges": {"color": {"inherit": true}, "smooth": false}, "physics": {"forceAtlas2Based": {"gravitationalConstant": -150, "springLength": 200}, "minVelocity": 0.75, "solver": "forceAtlas2Based"}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>